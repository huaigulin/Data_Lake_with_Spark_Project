# Data Engineering Project 4: Data Lake with Spark

- The purpose of the project is to apply what I have learnt on data lake, Spark and AWS, and build an ETL pipeline for a data lake hosted on Amazon S3.

## Background

- An imaginary startup called Sparkify has a new music streaming app, and the company's data analytics team is interested in knowing what songs its users are listening to.

- Sparkify has two datasets before this project:

  1. **The song dataset** is a subset of data from [Million Song Dataset](http://millionsongdataset.com/). It consists of JSON files and each contains metadata about a song and its artist.

  1. **The log dataset** is generated by this [event simulator](https://github.com/Interana/eventsim), and it simulates user activity logs from the music streaming app.

- Since it is not easy to query song play data from these two datasets, my job is using Spark to build an ETL pipeline to create a database in the form of parquet files on Amazon S3 to help the data analytics team analyze song play data.

## Database Schema

- For the purpose of song play analysis, I create a [star schema](https://en.wikipedia.org/wiki/Star_schema) in which there are a fact table _songplays_ and four dimension tables _users_, _songs_, _artists_ and _time_:

  1. The _songplays_ table holds records in the log dataset that are associated with song plays i.e. records with page `NextSong`. The table has the following columns:

     | Column      | Data Type | Comment                                                                                                                                                                                                 |
     | ----------- | --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
     | songplay_id | long      | Auto-generated when a row is inserted into the table.                                                                                                                                                   |
     | start_time  | timestamp | The timestamp that the user starts to play the song, which corresponds to the `ts`(timestamp) data from **the log dataset**.                                                                            |
     | user_id     | string    | The user's id, which corresponds to the `userId` data from **the log dataset**.                                                                                                                         |
     | level       | string    | The user's level(subscription type), which corresponds to the `level` data from **the log dataset**.                                                                                                    |
     | song_id     | string    | The id of the song that's been played, which corresponds to the `song_id` data from **the song dataset** and can be retreived from _songs_ table after joining _songs_ and _artists_ tables.            |
     | artist_id   | string    | The artist id of the song that's been played, which corresponds to the `artist_id` data from **the song dataset** and can be retreived from _artists_ table after joining _songs_ and _artists_ tables. |
     | session_id  | long      | The id of the user's playing session, which corresponds to the `sessionId` data from **the log dataset**.                                                                                               |
     | location    | string    | The location of the user, which corresponds to the `location` data from **the log dataset**.                                                                                                            |
     | user_agent  | string    | The device that the user used to play the song, which corresponds to the `userAgent` data from **the log dataset**.                                                                                     |

  1. The _users_ table holds data for users of the app. It has the following columns:

     | Column     | Data Type |
     | ---------- | --------- |
     | user_id    | integer   |
     | first_name | string    |
     | last_name  | string    |
     | gender     | string    |
     | level      | string    |

  1. The _songs_ table holds data for all the songs in **the song dataset**. The columns are:

     | Column    | Data Type |
     | --------- | --------- |
     | song_id   | string    |
     | title     | string    |
     | artist_id | string    |
     | year      | integer   |
     | duration  | double    |

  1. The _artists_ table holds data for all the artists in **the song dataset**. The columns are:

     | Column    | Data Type |
     | --------- | --------- |
     | artist_id | string    |
     | name      | string    |
     | location  | string    |
     | latitude  | double    |
     | longitude | double    |

  1. The _time_ table is the timestamps of records in _songplays_ broken down into specific units. The columns are:

     | Column     | Data Type |
     | ---------- | --------- |
     | start_time | timestamp |
     | hour       | integer   |
     | day        | integer   |
     | week       | integer   |
     | month      | integer   |
     | year       | integer   |
     | weekday    | string    |


## ETL Pipeline

- `etl.py` file contains code for connecting to AWS EMR, creating a Spark session and running ETL pipeline, including extracting data from AWS S3, transforming data with Spark using schema on read technique, and loading data back into S3.

## How to Run

1. Create an IAM Role and a EMR cluster on AWS and get `ACCESS_KEY_ID` and `SECRET_ACCESS_KEY` from an IAM role that can access EMR. Fill out the information in `dwh.cfg` config file.

1. Run ETL pipeline:

   ```
   python etl.py
   ```

1. Wait until ETL pipelin finish running and check tables created in S3.
